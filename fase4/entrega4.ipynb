{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBU0An9AWZgl"
      },
      "source": [
        "**CONTEXTO DO PROJETO**\n",
        "\n",
        "Este sistema desenvolve uma solução completa para análise automatizada de conteúdo em vídeos, aplicando algoritmos avançados de visão computacional. A ferramenta implementa reconhecimento biométrico facial, interpretação de estados emocionais e classificação de comportamentos corporais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3pnA7RBWZgl"
      },
      "source": [
        "**ESPECIFICAÇÕES TÉCNICAS**\n",
        "\n",
        "O sistema implementa as seguintes funcionalidades principais:\n",
        "\n",
        "1. **Identificação Biométrica**: Localização e validação de estruturas faciais presentes na sequência de vídeo.\n",
        "2. **Interpretação Emocional**: Classificação automática de estados afetivos através da análise facial.\n",
        "3. **Classificação de Comportamentos**: Identificação e categorização de padrões de movimento corporal.\n",
        "4. **Síntese Automatizada**: Produção de relatório consolidado com métricas e descobertas principais.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChI2j9aKWZgl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhJniP6DWZgl"
      },
      "source": [
        "### **Configuração do Ambiente de Dependências**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oM79M2iWZgl"
      },
      "outputs": [],
      "source": [
        "# %pip install opencv-python mediapipe python-docx tqdm deepface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8Qpd7tnWZgm"
      },
      "source": [
        "### **Carregamento de Módulos Essenciais**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXnzLiiWWZgm",
        "outputId": "c1759f4e-ed29-49d1-fc29-cc41cb4d53ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\luizg\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from docx import Document\n",
        "from tqdm import tqdm\n",
        "from deepface import DeepFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZv1necCWZgm"
      },
      "source": [
        "### **Inicialização de Registros Estatísticos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLYyeyasWZgm"
      },
      "outputs": [],
      "source": [
        "# Estruturas para armazenamento de dados analíticos\n",
        "registro_faces_encontradas = {}\n",
        "contabilizador_sentimentos = {}\n",
        "mapeamento_atividades = {}\n",
        "contador_quadros = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgABplA2WZgn"
      },
      "source": [
        "### **Configuração dos Algoritmos de Detecção**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKGqpegHWZgn"
      },
      "outputs": [],
      "source": [
        "# Inicialização do sistema de detecção facial baseado em Haar Cascade\n",
        "detector_facial = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YxjLVueWZgn"
      },
      "outputs": [],
      "source": [
        "# Configuração do framework MediaPipe para análise postural e esquelética\n",
        "rastreador_pose = mp.solutions.pose\n",
        "sistema_pose = rastreador_pose.Pose()\n",
        "utilitarios_desenho = mp.solutions.drawing_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_P-wavlWZgn"
      },
      "source": [
        "### **Módulo de Identificação Biométrica Facial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm538ACEWZgn"
      },
      "outputs": [],
      "source": [
        "def identificar_faces(quadro_atual):\n",
        "    # Transformação para escala de cinza para otimização computacional\n",
        "    imagem_cinza = cv2.cvtColor(quadro_atual, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Aplicação do algoritmo de detecção com parâmetros otimizados\n",
        "    regioes_faciais = detector_facial.detectMultiScale(imagem_cinza, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    # Processamento individual de cada região candidata identificada\n",
        "    for (coord_x, coord_y, largura, altura) in regioes_faciais:\n",
        "        # Extração da área de interesse contendo características faciais\n",
        "        segmento_facial = quadro_atual[coord_y:coord_y+altura, coord_x:coord_x+largura]\n",
        "\n",
        "        # Verificação de integridade dos dados extraídos\n",
        "        if segmento_facial.size == 0 or coord_x < 0 or coord_y < 0:\n",
        "            registro_faces_encontradas[\"rejeitadas\"] += 1\n",
        "            continue\n",
        "\n",
        "        # Normalização dimensional e conversão cromática para compatibilidade\n",
        "        segmento_facial = cv2.resize(segmento_facial, (160, 160))\n",
        "        segmento_facial = cv2.cvtColor(segmento_facial, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        try:\n",
        "            # Geração de assinatura biométrica através do modelo FaceNet\n",
        "            assinatura_biometrica = DeepFace.represent(segmento_facial, model_name=\"Facenet\")\n",
        "\n",
        "            # Validação da qualidade da assinatura gerada\n",
        "            if assinatura_biometrica and len(assinatura_biometrica) > 0:\n",
        "                # Anotação visual de face válida identificada\n",
        "                cv2.rectangle(quadro_atual, (coord_x, coord_y), (coord_x+largura, coord_y+altura), (0, 255, 0), 2)\n",
        "                cv2.putText(quadro_atual, 'Confirmado', (coord_x, coord_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "                if registro_faces_encontradas.get(\"confirmadas\") is None:\n",
        "                    registro_faces_encontradas[\"confirmadas\"] = 1\n",
        "                else:\n",
        "                    registro_faces_encontradas[\"confirmadas\"] += 1\n",
        "            else:\n",
        "                if registro_faces_encontradas.get(\"rejeitadas\") is None:\n",
        "                    registro_faces_encontradas[\"rejeitadas\"] = 1\n",
        "                else:\n",
        "                    registro_faces_encontradas[\"rejeitadas\"] += 1\n",
        "        except Exception as erro:\n",
        "            # Tratamento de exceções durante o processamento biométrico\n",
        "            if registro_faces_encontradas.get(\"rejeitadas\") is None:\n",
        "                registro_faces_encontradas[\"rejeitadas\"] = 1\n",
        "            else:\n",
        "                registro_faces_encontradas[\"rejeitadas\"] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpHURrkBWZgn"
      },
      "source": [
        "### **Sistema de Interpretação de Estados Emocionais**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrOR2LpqWZgn"
      },
      "outputs": [],
      "source": [
        "def analisar_sentimentos(quadro_atual):\n",
        "    # Execução da análise emocional através de redes neurais especializadas\n",
        "    resultado_analise = DeepFace.analyze(quadro_atual, actions=['emotion'], enforce_detection=False)\n",
        "\n",
        "    # Iteração através de cada face identificada no quadro atual\n",
        "    for estrutura_facial in resultado_analise:\n",
        "        # Aplicação de filtro de confiança para garantir precisão\n",
        "        if estrutura_facial['face_confidence'] > 0.9:\n",
        "            pos_x, pos_y, dim_w, dim_h = estrutura_facial['region']['x'], estrutura_facial['region']['y'], estrutura_facial['region']['w'], estrutura_facial['region']['h']\n",
        "            estado_emocional_primario = estrutura_facial['dominant_emotion']\n",
        "\n",
        "            # Acumulação estatística dos estados emocionais identificados\n",
        "            if estado_emocional_primario in contabilizador_sentimentos:\n",
        "                contabilizador_sentimentos[estado_emocional_primario] += 1\n",
        "            else:\n",
        "                contabilizador_sentimentos[estado_emocional_primario] = 1\n",
        "\n",
        "            # Renderização visual da classificação emocional\n",
        "            cv2.rectangle(quadro_atual, (pos_x, pos_y), (pos_x+dim_w, pos_y+dim_h), (0, 255, 0), 2)\n",
        "            cv2.putText(quadro_atual, estado_emocional_primario, (pos_x, pos_y+dim_h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sIt-krFWZgn"
      },
      "source": [
        "### **Algoritmo de Classificação de Comportamentos Corporais**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLLimJduWZgn"
      },
      "outputs": [],
      "source": [
        "def mapear_posturas(quadro_atual):\n",
        "    # Conversão cromática para compatibilidade com MediaPipe\n",
        "    quadro_rgb = cv2.cvtColor(quadro_atual, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Aplicação do modelo de detecção esquelética\n",
        "    dados_posturais = sistema_pose.process(quadro_rgb)\n",
        "\n",
        "    # Renderização das conexões esqueléticas identificadas\n",
        "    if dados_posturais.pose_landmarks:\n",
        "        utilitarios_desenho.draw_landmarks(quadro_atual, dados_posturais.pose_landmarks, rastreador_pose.POSE_CONNECTIONS)\n",
        "\n",
        "        # Extração dos pontos anatômicos de referência\n",
        "        pontos_corporais = dados_posturais.pose_landmarks.landmark\n",
        "\n",
        "        # Mapeamento de articulações relevantes para análise comportamental\n",
        "        articulacao_ombro_esq = pontos_corporais[rastreador_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
        "        articulacao_ombro_dir = pontos_corporais[rastreador_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
        "        articulacao_punho_esq = pontos_corporais[rastreador_pose.PoseLandmark.LEFT_WRIST.value]\n",
        "        articulacao_punho_dir = pontos_corporais[rastreador_pose.PoseLandmark.RIGHT_WRIST.value]\n",
        "\n",
        "        # Identificação do ponto de referência cefálico\n",
        "        referencia_nasal = pontos_corporais[rastreador_pose.PoseLandmark.NOSE.value]\n",
        "\n",
        "        # Classificação de gestos através de análise posicional relativa\n",
        "        if articulacao_punho_esq.y < articulacao_ombro_esq.y:\n",
        "            if mapeamento_atividades.get(\"elevacao_membro_esquerdo\") is None:\n",
        "                mapeamento_atividades[\"elevacao_membro_esquerdo\"] = 1\n",
        "            else:\n",
        "                mapeamento_atividades[\"elevacao_membro_esquerdo\"] += 1\n",
        "        elif articulacao_punho_dir.y < articulacao_ombro_dir.y:\n",
        "            if mapeamento_atividades.get(\"elevacao_membro_direito\") is None:\n",
        "                mapeamento_atividades[\"elevacao_membro_direito\"] = 1\n",
        "            else:\n",
        "                mapeamento_atividades[\"elevacao_membro_direito\"] += 1\n",
        "        # Detecção de proximidade mão-face\n",
        "        elif articulacao_punho_esq.y < referencia_nasal.y:\n",
        "            if mapeamento_atividades.get(\"contato_mao_esquerda_facial\") is None:\n",
        "                mapeamento_atividades[\"contato_mao_esquerda_facial\"] = 1\n",
        "            else:\n",
        "                mapeamento_atividades[\"contato_mao_esquerda_facial\"] += 1\n",
        "        elif articulacao_punho_dir.y < referencia_nasal.y:\n",
        "            if mapeamento_atividades.get(\"contato_mao_direita_facial\") is None:\n",
        "                mapeamento_atividades[\"contato_mao_direita_facial\"] = 1\n",
        "            else:\n",
        "                mapeamento_atividades[\"contato_mao_direita_facial\"] += 1\n",
        "        # Categorização de padrões não classificados\n",
        "        else:\n",
        "            if mapeamento_atividades.get(\"comportamento_indefinido\") is None:\n",
        "                mapeamento_atividades[\"comportamento_indefinido\"] = 1\n",
        "            else:\n",
        "                mapeamento_atividades[\"comportamento_indefinido\"] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r9OjRHTWZgn"
      },
      "source": [
        "### **Gerador de Documentação Analítica**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2B_PbEkWZgn"
      },
      "outputs": [],
      "source": [
        "def criar_relatorio():\n",
        "    # Instanciação do processador de documentos\n",
        "    documento_final = Document()\n",
        "\n",
        "    # Estruturação do cabeçalho principal\n",
        "    documento_final.add_heading('Relatório Analítico de Processamento Audiovisual', level=1)\n",
        "\n",
        "    # Seção de métricas de processamento\n",
        "    documento_final.add_heading('Estatísticas de Processamento', level=2)\n",
        "    documento_final.add_paragraph(f\"Quadros processados: {contador_quadros}\")\n",
        "\n",
        "    documento_final.add_heading('Detecções Biométricas', level=2)\n",
        "    for categoria, quantidade in registro_faces_encontradas.items():\n",
        "        documento_final.add_paragraph(f\"{categoria}: {quantidade}\")\n",
        "\n",
        "    documento_final.add_heading('Análise Emocional', level=2)\n",
        "    for estado, frequencia in contabilizador_sentimentos.items():\n",
        "        documento_final.add_paragraph(f\"{estado}: {frequencia}\")\n",
        "\n",
        "    documento_final.add_heading('Classificação Comportamental', level=2)\n",
        "    for acao, ocorrencias in mapeamento_atividades.items():\n",
        "        documento_final.add_paragraph(f\"{acao}: {ocorrencias}\")\n",
        "\n",
        "    # Persistência do documento gerado\n",
        "    documento_final.save(\"relatorio_processamento_audiovisual.docx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZGyOMkTWZgn"
      },
      "source": [
        "### **Sistema Principal de Processamento Multimídia**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkpH5RefWZgn"
      },
      "outputs": [],
      "source": [
        "def processar_midia(arquivo_entrada, arquivo_destino):\n",
        "    global contador_quadros\n",
        "\n",
        "    # Inicialização do stream de vídeo\n",
        "    stream_video = cv2.VideoCapture(arquivo_entrada)\n",
        "\n",
        "    if not stream_video.isOpened():\n",
        "        print(\"Falha na inicialização do stream de vídeo\")\n",
        "        return\n",
        "    else:\n",
        "        print(\"Stream de vídeo estabelecido com êxito\")\n",
        "\n",
        "    # Extração de metadados do arquivo de entrada\n",
        "    resolucao_horizontal = int(stream_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    resolucao_vertical = int(stream_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    taxa_quadros = stream_video.get(cv2.CAP_PROP_FPS)\n",
        "    total_quadros_video = int(stream_video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Configuração do intervalo de amostragem\n",
        "    frequencia_processamento = 1\n",
        "\n",
        "    # Configuração do codificador de saída\n",
        "    codec_compressao = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    escritor_video = cv2.VideoWriter(arquivo_destino, codec_compressao, taxa_quadros, (resolucao_horizontal, resolucao_vertical))\n",
        "\n",
        "    # Loop principal de processamento\n",
        "    for _ in tqdm(range(total_quadros_video), desc=\"Executando análise audiovisual\"):\n",
        "        sucesso_leitura, quadro_buffer = stream_video.read()\n",
        "\n",
        "        if not sucesso_leitura:\n",
        "            break\n",
        "\n",
        "        # Aplicação seletiva dos algoritmos de análise\n",
        "        if contador_quadros % frequencia_processamento == 0:\n",
        "            # Execução da identificação biométrica\n",
        "            identificar_faces(quadro_buffer)\n",
        "\n",
        "            # Execução da análise emocional\n",
        "            analisar_sentimentos(quadro_buffer)\n",
        "\n",
        "            # Execução da classificação postural\n",
        "            mapear_posturas(quadro_buffer)\n",
        "\n",
        "        # Persistência do quadro processado\n",
        "        escritor_video.write(quadro_buffer)\n",
        "\n",
        "        # Incremento do contador de progresso\n",
        "        contador_quadros += 1\n",
        "\n",
        "    # Liberação de recursos do sistema\n",
        "    stream_video.release()\n",
        "    escritor_video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    print(\"Processamento audiovisual finalizado com sucesso\")\n",
        "\n",
        "    # Geração do relatório consolidado\n",
        "    criar_relatorio()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxJtYMpqWZgn"
      },
      "source": [
        "### **Execução do Pipeline de Análise**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-BFWEFtWZgo",
        "outputId": "fe37ddf0-b463-4925-ee64-b99a2133b93c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stream de vídeo estabelecido com êxito\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Executando análise audiovisual: 100%|██████████| 3326/3326 [22:07<00:00,  2.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processamento audiovisual finalizado com sucesso\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Definição dos caminhos de entrada e saída\n",
        "arquivo_midia_origem = 'video_tech_challenge.mp4'\n",
        "arquivo_midia_processada = 'analise.mp4'\n",
        "\n",
        "# Inicialização do processamento\n",
        "processar_midia(arquivo_midia_origem, arquivo_midia_processada)"
      ]
    }
  ],
  "metadata": {
    "kernelnel": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}